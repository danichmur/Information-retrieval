{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Индексация\n",
    "Реализуйте построение инвертированного индекса в памяти для коллекции из домашней работы номер 3. В каждом постинглисте также сохраните значение term-frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "ArticleName = str\n",
    "Text = str\n",
    "Term = str\n",
    "CollectionData = Dict[str, Dict[str,  float]]\n",
    "RankingParams = {}\n",
    "\n",
    "PREFIX = '/wiki/'\n",
    "AVERAGE_KEY = 'avg_len'\n",
    "ID_TO_TITLE_KEY = 'id2title'\n",
    "DOC_TO_LEN_KEY = 'doc2len'\n",
    "POSTINGS_LEN_KEY = 'postings_len'\n",
    "\n",
    "DocId = int\n",
    "TermFreq = int\n",
    "\n",
    "RelevInfo = float\n",
    "\n",
    "Posting = List[Tuple[DocId, RelevInfo]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import html2text\n",
    "\n",
    "url_path_map_value = {}\n",
    "\n",
    "with open('url_path_map.p', 'rb') as f:\n",
    "    url_path_map_value = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/danielmuraveyko/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re \n",
    "import itertools  \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def html2text(html):\n",
    "    soup = BeautifulSoup(html)\n",
    "    paras = []\n",
    "    for paragraph in soup.find_all('p'):\n",
    "        paras.append(str(paragraph.text))\n",
    "    heads = []\n",
    "    for head in soup.find_all('span', attrs={'mw-headline'}):\n",
    "        heads.append(str(head.text))\n",
    "    text = [val for pair in itertools.zip_longest(paras, heads, fillvalue =' ' ) for val in pair]\n",
    "    text = ' '.join(text)\n",
    "    text = re.sub(r\"\\[.*?\\]+\", '', text)\n",
    "\n",
    "    text = text.replace('\\n', '')[:-11]\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_article_text(article_name: ArticleName) -> Text:\n",
    "    text = \"\"\n",
    "    key = PREFIX + article_name\n",
    "    if key in url_path_map_value:\n",
    "        path = url_path_map_value[key]\n",
    "        if path != '':\n",
    "            with gzip.open(path, 'rb') as f:\n",
    "                html = gzip.decompress(f.read()).decode('utf-8')\n",
    "                title = article_name.replace('_', ' ')\n",
    "                text = f'{title} {html2text(html)}'\n",
    "    return text\n",
    "\n",
    "import string\n",
    "\n",
    "def make_terms(text: Text) -> List[Term]:\n",
    "    words = text.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    return [t for t in words if t not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15229it [09:47, 25.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15229 docs loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#with multi-processes: AttributeError: Can't get attribute 'get_article_text' on <module '__main__' (built-in)> !!!!\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def load_docs(selected_docs_fn: ArticleName, threads: int = 4) -> Dict[ArticleName, Text]:    \n",
    "    docs = {}\n",
    "    for line in tqdm(open(selected_docs_fn)):\n",
    "        article_name = line.strip()\n",
    "        docs[article_name] = get_article_text(article_name)\n",
    "    return docs\n",
    "    \n",
    "docs = load_docs(\"./selected_docs.tsv\", 32)\n",
    "print(f'{len(docs)} docs loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bags_of_words = {k:make_terms(v) for k, v in docs.items()}\n",
    "\n",
    "def invert_index(docs: Dict[ArticleName, Text]) -> Tuple[Dict[Term, Posting], Dict[ArticleName, DocId]]:\n",
    "    \n",
    "    num_of_words = {k:dict(Counter(v)) for k, v in bags_of_words.items()}\n",
    "    index = {}\n",
    "    \n",
    "    article2doc_id = {title:i for i, (title, body) in enumerate(docs.items())}\n",
    "\n",
    "    for title, tf in num_of_words.items():\n",
    "        article_id = article2doc_id[title]\n",
    "        for word, val in tf.items():\n",
    "            d = index.get(word, [])\n",
    "            d.append((article_id, val))\n",
    "            index[word] = d\n",
    "    \n",
    "    return index, article2doc_id\n",
    "    \n",
    "    \n",
    "\n",
    "index, article_to_doc_id = invert_index(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id2article = {v:k for k, v in article_to_doc_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9972248"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(posting) for _, posting in index.items()]) * 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраните полученный индекс на диске в бинарном формате. Формат должен позволять читать отсортированные по термам постинглисты, по одному за раз. Размер сохраненного индекса в байтах должен быть порядка 8*(сумму длин всех постинг листов). \n",
    "\n",
    "Отдельно сохраните на диск дополнительные данные о коллекции, которые пригодятся для поиска, например названия статей или среднюю длину документа. Размер дополнительных данных, должен быть пропорционален количеству документов коллекции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index file size: 11980370\n",
      "Collection data file size: 2062692\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "def dump_index(index: Dict[Term, Posting], filename: str) -> None:\n",
    "    with open(filename, 'wb') as out_file:\n",
    "        \n",
    "        out_file.write(struct.pack(\"i\", len(index)))\n",
    "        for term, posting in index.items():\n",
    "            bytes_term = term.encode()\n",
    "            p_len = len(posting)\n",
    "            doc_ids = [p[0] for p in posting]\n",
    "            tfs = [p[1] for p in posting]\n",
    "\n",
    "            out_file.write(struct.pack(\"i\", len(bytes_term)))\n",
    "            out_file.write(struct.pack(f\"{len(bytes_term)}s\", bytes_term))\n",
    "            out_file.write(struct.pack(\"i\", p_len))\n",
    "            out_file.write(struct.pack(f\"{p_len}i\", *doc_ids))\n",
    "            out_file.write(struct.pack(f\"{p_len}i\", *tfs))\n",
    "\n",
    "\n",
    "\n",
    "def dump_collectiondata(data: CollectionData, filename: str) -> None:\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    \n",
    "dump_index(index, \"index.inv\")\n",
    "print(\"Index file size:\", os.path.getsize(\"index.inv\"))\n",
    "\n",
    "docs_len = {article_to_doc_id[title]:len(body) for title, body in bags_of_words.items()}\n",
    "bodies = docs_len.values()\n",
    "average_len = sum(bodies) / len(bodies)\n",
    "postings_len = {k:len(v) for k, v in index.items()}\n",
    "\n",
    "collection_data = {\n",
    "    AVERAGE_KEY:average_len,\n",
    "    ID_TO_TITLE_KEY:doc_id2article,\n",
    "    DOC_TO_LEN_KEY:docs_len,\n",
    "    POSTINGS_LEN_KEY:postings_len\n",
    "}\n",
    "\n",
    "dump_collectiondata(collection_data, \"index.data\")\n",
    "print(\"Collection data file size:\", os.path.getsize(\"index.data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Поиск\n",
    "\n",
    "Для простоты реализации поиска, не требуется делать чтение постинглистов с диска по запросу - достаточно считать их с диска в память целиком. Также загрузите с диска дополнительные данные о коллекции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number or terms in index: 124173\n"
     ]
    }
   ],
   "source": [
    "def load_index(filename: str) -> Dict[Term, Posting]:\n",
    "    index = {}\n",
    "    \n",
    "    with open(filename, \"rb\") as in_file:\n",
    "        size = struct.unpack(\"i\", in_file.read(4))[0]\n",
    "        for _ in range(size):\n",
    "            term_len = struct.unpack(\"i\", in_file.read(4))[0]\n",
    "            term_b = in_file.read(term_len)\n",
    "            term = struct.unpack(f\"{term_len}s\", term_b)[0].decode()\n",
    "\n",
    "            bytes_arr_len = struct.unpack(\"i\", in_file.read(4))[0]\n",
    "            doc_ids_b = in_file.read(bytes_arr_len * 4)\n",
    "            tfs_b = in_file.read(bytes_arr_len * 4)\n",
    "            doc_ids = struct.unpack(f\"{bytes_arr_len}i\", doc_ids_b)\n",
    "            tfs = struct.unpack(f\"{bytes_arr_len}i\", tfs_b)\n",
    "            index[term] = list(zip(doc_ids, tfs))\n",
    "\n",
    "    return index        \n",
    "        \n",
    "\n",
    "def load_collectiondata(filename: str) -> CollectionData:\n",
    "    data_new = {}\n",
    "    with open(filename, 'rb') as f:\n",
    "        data_new = pickle.load(f)\n",
    "    return data_new\n",
    "\n",
    "\n",
    "index0 = load_index(\"index.inv\")\n",
    "print(\"Number or terms in index:\", len(index))\n",
    "collection_data = load_collectiondata(\"index.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert index0 == index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 queries loaded\n",
      "animals that have shells and live in water -> Shell_(zoology)\n",
      "how many different types of scorpions are there -> Scorpion\n",
      "describe the structure of a scientific name for a species -> Binomial_nomenclature\n",
      "what are the 3 types of plastids in plant cells -> Plastid\n",
      "who named the cell and how did he come up with that name -> Cell_theory\n"
     ]
    }
   ],
   "source": [
    "def load_queries(queries_fn: ArticleName) -> List[Tuple[Text, ArticleName]]:\n",
    "    queries = []\n",
    "    for line in open(queries_fn):\n",
    "        query, answer = line.rstrip().split('\\t', 1)\n",
    "        queries.append((query, answer))\n",
    "    return queries\n",
    "\n",
    "queries = load_queries(\"./queries.tsv\")\n",
    "for query, answer in queries:\n",
    "    assert answer in docs\n",
    "    \n",
    "print(f'{len(queries)} queries loaded')\n",
    "for query, article_name in queries[:5]:\n",
    "    print(f'{query} -> {article_name}')\n",
    "    \n",
    "def run(title, search, queries: List[Tuple[Text, ArticleName]], index: Dict[Term, Posting], collection_data: CollectionData, ranking_params: RankingParams) -> None:\n",
    "    accuracy = 0.0\n",
    "    accuracy10 = 0.0\n",
    "    rr = 0.0\n",
    "    processed = 0\n",
    "    with tqdm(queries) as progress:\n",
    "        for query, answer in progress:\n",
    "            result = search(query, 10, index, collection_data, ranking_params)\n",
    "            \n",
    "            rank = None\n",
    "            for position, (article_name, score) in enumerate(result):\n",
    "                if article_name == answer:\n",
    "                    rank = position + 1\n",
    "                    break\n",
    "                \n",
    "            if rank is not None:\n",
    "                accuracy += (rank == 1)\n",
    "                accuracy10 += (rank <= 10)\n",
    "                rr += 1.0 / rank\n",
    "                \n",
    "            processed += 1\n",
    "            progress.set_description(f'Acc: {accuracy/processed:0.2f}, Acc10: {accuracy10/processed:0.2f}, RR: {rr/processed:0.2f}')\n",
    "    print(f'{title}\\n  Accuracy: {accuracy/processed:0.2f}\\n  Accuracy10: {accuracy10/processed:0.2f}\\n  RR: {rr/processed:0.2f}')\n",
    "    \n",
    "\n",
    "\n",
    "def demo_search(index, search):\n",
    "    ranking_params = {'b': 0.5, 'k1': 4, 'k2': 0}\n",
    "\n",
    "    for query in [\"coronovirus in belarus\",\n",
    "                  \"who won junior eurovision in 2005\",\n",
    "                  \"science about full-text search\",\n",
    "                 ]:\n",
    "        result = search(query, 5, index, collection_data, ranking_params)[:5]\n",
    "        print(f\"[{query}]\")\n",
    "        for article_name, score in result:\n",
    "            print(f\"{score:7.2f}  {article_name}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def score_BM25(N, average_len, n, f, qf, dl, b, k1, k2):\n",
    "    K = compute_K(k1, b, dl, average_len)\n",
    "    first = math.log((N - n + 0.5) / (n + 0.5))\n",
    "    second = ((k1 + 1) * f) / (K + f)\n",
    "    third = ((k2+1) * qf) / (k2 + qf)\n",
    "    return first * second * third\n",
    "\n",
    "\n",
    "def compute_K(k1, b, dl, avdl):\n",
    "    return k1 * ((1 - b) + b * dl / avdl)\n",
    "\n",
    "def search_indexed(query: Text, top_size: int, index: Dict[Term, Posting], collection_data: CollectionData, ranking_params: RankingParams) -> List[Tuple[ArticleName, float]]:\n",
    "    terms = make_terms(query)\n",
    "    print(terms)\n",
    "\n",
    "    q = dict(Counter(terms))\n",
    "    c = Counter({})\n",
    "\n",
    "\n",
    "    b = ranking_params.get('b', 1)\n",
    "    k1 = ranking_params.get('k1', 1)\n",
    "    k2 = ranking_params.get('k2', 1)\n",
    "\n",
    "    lens = collection_data.get(DOC_TO_LEN_KEY, {})\n",
    "    average_len = collection_data.get(AVERAGE_KEY, 1)\n",
    "    doc_id2article = collection_data.get(ID_TO_TITLE_KEY, {})\n",
    "    postings_len = collection_data.get(POSTINGS_LEN_KEY, {})\n",
    "    \n",
    "    for term in terms:\n",
    "        posting = index.get(term, [])\n",
    "        for doc_id, freq in posting:\n",
    "            \n",
    "                \n",
    "            score = score_BM25(N=len(index), average_len=average_len, n=postings_len.get(term, 1),\n",
    "                               f=freq, qf=q[term],\n",
    "                               dl=lens.get(doc_id, 1),\n",
    "                               b=b, k1=k1, k2=k2\n",
    "            )\n",
    "            if doc_id in c:\n",
    "                c[doc_id] += score\n",
    "            else:\n",
    "                c[doc_id] = score\n",
    "    res = sorted([(doc_id2article.get(doc_id, \"\"), rank) for doc_id, rank in c.items()], key=lambda x: -x[1])\n",
    "    return res[:top_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coronovirus', 'belarus']\n",
      "[coronovirus in belarus]\n",
      "  25.27  COVID-19_pandemic_in_Belarus\n",
      "  17.68  Daugava_River\n",
      "  12.16  Bug_River\n",
      "   9.94  Jagiellon_dynasty\n",
      "   9.68  Byelorussian_Soviet_Socialist_Republic\n",
      "\n",
      "\n",
      "['junior', 'eurovision', '2005']\n",
      "[who won junior eurovision in 2005]\n",
      "  43.55  Junior_Eurovision_Song_Contest_2019\n",
      "  34.41  Junior_Eurovision_Song_Contest_2004\n",
      "  34.21  Junior_Eurovision_Song_Contest_2014\n",
      "  34.01  Junior_Eurovision_Song_Contest_2015\n",
      "  30.98  List_of_ice_hockey_leagues\n",
      "\n",
      "\n",
      "['science', 'fulltext', 'search']\n",
      "[science about full-text search]\n",
      "  28.45  Information_retrieval\n",
      "  26.80  Google_Search\n",
      "  22.92  Popular_science\n",
      "  22.74  Citizen_science\n",
      "  21.99  Philosophy_of_science\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demo_search(index0, search_indexed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте поиск документов с ранжированием BM25 на основе инвертированного индекса в парадигме document-at-time, то есть через [слияние](https://en.wikipedia.org/wiki/Merge_algorithm) постинглистов. Функция поиска должна принимать число - ограничение на количество документов, возвращаемое поиском. Используемое количество дополнительной памяти должно быть пропорционально этому ограничению и никак не должно зависить от размера постинглистов или размера коллекции.\n",
    "Результаты поиска должны быть аналогичные тем, что были в домашней работе номер 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import heapify, heappush, heappushpop, heappop\n",
    "   \n",
    "def search_indexed_heaps(query: Text, top_size: int, index: Dict[Term, Posting], collection_data: CollectionData, ranking_params: RankingParams) -> List[Tuple[ArticleName, float]]:\n",
    "    terms = make_terms(query)\n",
    "    q = dict(Counter(terms))\n",
    "    c = Counter({})\n",
    "\n",
    "\n",
    "    b = ranking_params.get('b', 1)\n",
    "    k1 = ranking_params.get('k1', 1)\n",
    "    k2 = ranking_params.get('k2', 1)\n",
    "\n",
    "    lens = collection_data.get(DOC_TO_LEN_KEY, {})\n",
    "    average_len = collection_data.get(AVERAGE_KEY, 1)\n",
    "    doc_id2article = collection_data.get(ID_TO_TITLE_KEY, {})\n",
    "    postings_len = collection_data.get(POSTINGS_LEN_KEY, {})\n",
    "\n",
    "    terms_heap = []\n",
    "    heapify(terms_heap)\n",
    "    terms_heap_len = len(q)\n",
    "\n",
    "    for term in terms:\n",
    "        if term in index:\n",
    "            posting = index[term]\n",
    "            doc_id, tf = posting[0]\n",
    "            elem = (doc_id, (term, tf), 1)\n",
    "            heappush(terms_heap, elem) if len(terms_heap) < terms_heap_len else heappushpop(terms_heap, elem)\n",
    "    \n",
    "    doc_id, (_, _), idx = terms_heap[0]\n",
    "    \n",
    "    result = []\n",
    "    heapify(result)\n",
    "    \n",
    "    while len(terms_heap) > 0:\n",
    "        tfs = {}\n",
    "        while len(terms_heap) > 0:\n",
    "            min_id, (term, tf), next_idx = terms_heap[0]\n",
    "            \n",
    "            if min_id != doc_id:\n",
    "                break\n",
    "            heappop(terms_heap)\n",
    "            \n",
    "            tfs[term] = tf\n",
    "\n",
    "            if next_idx > len(index[term]) - 1:\n",
    "                continue\n",
    "\n",
    "            min_id, tf = index[term][next_idx]\n",
    "            elem = (min_id, (term, tf), next_idx + 1)\n",
    "            heappush(terms_heap, elem) if len(terms_heap) < terms_heap_len else heappushpop(terms_heap, elem)\n",
    "        \n",
    "        bm25 = sum([\n",
    "            score_BM25(\n",
    "                N=len(index), average_len=average_len, n=postings_len.get(term, 1),\n",
    "                f=tfs.get(term, 0), qf=qf,\n",
    "                dl=lens.get(doc_id, 1),\n",
    "                b=b, k1=k1, k2=k2\n",
    "            )\n",
    "            for term, qf in q.items()\n",
    "        ])\n",
    "        \n",
    "        elem = (bm25, doc_id)\n",
    "        heappush(result, elem) if len(result) < top_size else heappushpop(result, elem)\n",
    "                \n",
    "        doc_id = min_id\n",
    "\n",
    "    return sorted([(doc_id2article[doc_id], score) for score, doc_id in result], key=lambda x: -x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[coronovirus in belarus]\n",
      "  25.27  COVID-19_pandemic_in_Belarus\n",
      "  17.68  Daugava_River\n",
      "  12.16  Bug_River\n",
      "   9.94  Jagiellon_dynasty\n",
      "   9.68  Byelorussian_Soviet_Socialist_Republic\n",
      "\n",
      "\n",
      "[who won junior eurovision in 2005]\n",
      "  43.55  Junior_Eurovision_Song_Contest_2019\n",
      "  34.41  Junior_Eurovision_Song_Contest_2004\n",
      "  34.21  Junior_Eurovision_Song_Contest_2014\n",
      "  34.01  Junior_Eurovision_Song_Contest_2015\n",
      "  30.98  List_of_ice_hockey_leagues\n",
      "\n",
      "\n",
      "[science about full-text search]\n",
      "  28.45  Information_retrieval\n",
      "  26.80  Google_Search\n",
      "  22.92  Popular_science\n",
      "  22.74  Citizen_science\n",
      "  21.99  Philosophy_of_science\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demo_search(index0, search_indexed_heaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте static pruning до 50 элементов для каждого постинглиста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого терма оставляем 50 документов с наибольшей частотой терма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune(index: Dict[Term, Posting], top_size: int = 50) -> Dict[Term, Posting]:\n",
    "    return {k:sorted(sorted(v, key=lambda x:-x[1])[:top_size], key=lambda x:x[0]) for k, v in index.items()}\n",
    "\n",
    "\n",
    "pruned_index = prune(index, 50)\n",
    "for term, posting in pruned_index.items():\n",
    "    prev_doc_id = -1\n",
    "    for doc_id, freq in posting:\n",
    "        assert doc_id > prev_doc_id\n",
    "        prev_doc_id = doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[coronovirus in belarus]\n",
      "  25.27  COVID-19_pandemic_in_Belarus\n",
      "  17.68  Daugava_River\n",
      "  12.16  Bug_River\n",
      "   9.94  Jagiellon_dynasty\n",
      "   9.68  Byelorussian_Soviet_Socialist_Republic\n",
      "\n",
      "\n",
      "[who won junior eurovision in 2005]\n",
      "  43.55  Junior_Eurovision_Song_Contest_2019\n",
      "  34.41  Junior_Eurovision_Song_Contest_2004\n",
      "  34.21  Junior_Eurovision_Song_Contest_2014\n",
      "  34.01  Junior_Eurovision_Song_Contest_2015\n",
      "  30.98  List_of_ice_hockey_leagues\n",
      "\n",
      "\n",
      "[science about full-text search]\n",
      "  26.80  Google_Search\n",
      "  23.00  Information_retrieval\n",
      "  22.92  Popular_science\n",
      "  21.28  British_Science_Association\n",
      "  20.53  Science_fiction_movie\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demo_search(pruned_index, search_indexed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравните качество и скорость работы нового алгоритма поиска с предыдущим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Acc: 0.23, Acc10: 0.48, RR: 0.31: 100%|██████████| 200/200 [00:02<00:00, 73.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25\n",
      "  Accuracy: 0.23\n",
      "  Accuracy10: 0.48\n",
      "  RR: 0.31\n",
      "--- 2.7399349212646484 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "run(\"BM25\", search_indexed, queries, index, collection_data, ranking_params)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Acc: 0.23, Acc10: 0.50, RR: 0.31: 100%|██████████| 200/200 [00:06<00:00, 29.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 doc-at-time\n",
      "  Accuracy: 0.23\n",
      "  Accuracy10: 0.50\n",
      "  RR: 0.31\n",
      "--- 6.747586250305176 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "ranking_params = {'b': 0.5, 'k1': 4, 'k2': 0}\n",
    "run(\"BM25 doc-at-time\", search_indexed_heaps, queries, index, collection_data, ranking_params)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравните качество и скорость работы нового индекса с предыдущим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Acc: 0.19, Acc10: 0.39, RR: 0.26: 100%|██████████| 200/200 [00:01<00:00, 146.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned BM25 doc-at-time\n",
      "  Accuracy: 0.19\n",
      "  Accuracy10: 0.39\n",
      "  RR: 0.26\n",
      "--- 1.3672301769256592 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "run(\"Pruned BM25 doc-at-time\", search_indexed_heaps, queries, pruned_index, collection_data, ranking_params)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ~Дополнительно~\n",
    "### Сжатие индекса (+1 балл)\n",
    "Реализуйте кодирование чисел алгоритмом VarInt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x01\\x02\\x03\\xac\\x02\\xa0\\x9c\\x01'\n"
     ]
    }
   ],
   "source": [
    "import struct \n",
    "import io\n",
    "\n",
    "\n",
    "class baseline_coder:\n",
    "    def encode(output_stream, posting):\n",
    "        for doc_id, freq in posting:\n",
    "            output_stream.write(struct.pack('II', doc_id, freq))\n",
    "            \n",
    "    def decode(input_stream):\n",
    "        res = []\n",
    "        while True:\n",
    "            data = input_stream.read(struct.calcsize('II'))\n",
    "            if len(data) == 0:\n",
    "                break\n",
    "            res.append(struct.unpack('II', data))\n",
    "        return res\n",
    "\n",
    "\n",
    "class varint_coder:\n",
    "    def encode_num(number):\n",
    "        buf = b''\n",
    "        while True:\n",
    "            towrite = number & 0x7f\n",
    "            number >>= 7\n",
    "            if number:\n",
    "                buf += (towrite | 0x80).to_bytes(1, byteorder='big')\n",
    "            else:\n",
    "                buf += (towrite).to_bytes(1, byteorder='big')\n",
    "                break\n",
    "        return buf\n",
    "    \n",
    "    def decode_num(input_stream):\n",
    "        shift = 0\n",
    "        result = 0\n",
    "        while True:\n",
    "            i = ord(input_stream.read(1))\n",
    "            result |= (i & 0x7f) << shift\n",
    "            shift += 7\n",
    "            if not (i & 0x80):\n",
    "                break\n",
    "\n",
    "        return shift / 7, result\n",
    "    \n",
    "    def decode(input_stream):\n",
    "        size = len(input_stream.getvalue())\n",
    "        total_read = 0\n",
    "        res = []\n",
    "        while total_read < size:\n",
    "            read, data = varint_coder.decode_num(input_stream)\n",
    "            total_read += read\n",
    "            res.append(data)\n",
    "        return res\n",
    "        \n",
    "    def encode(output_stream, posting):\n",
    "        for num in posting:\n",
    "            output_stream.write(varint_coder.encode_num(num))\n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "output = io.BytesIO()\n",
    "varint_coder.encode(output, [1, 2, 3, 300, 20000])\n",
    "print(output.getvalue())\n",
    "posting = varint_coder.decode(io.BytesIO(output.getvalue()))\n",
    "\n",
    "assert posting == [1, 2, 3, 300, 20000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравните эффективность разных вариантов кодирования постинглистов:\n",
    " - Базовый вариант (4 байта на число)\n",
    " - Какой-нибудь алгоритм сжатия общего назначения (lz4/zstd/brotli/gzip)\n",
    " - VarInt\n",
    " - Delta-кодирование + Какой-нибудь алгоритм сжатия общего назначения \n",
    " - Delta-кодирование + VarInt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lz4.frame\n",
    "\n",
    "class lz4_baseline_coder:\n",
    "    def encode(output_stream, posting):\n",
    "        output_stream0 = io.BytesIO()\n",
    "        baseline_coder.encode(output_stream0, posting)\n",
    "        output_stream.write(lz4.frame.compress(output_stream0.getvalue()))\n",
    "            \n",
    "    def decode(input_stream):\n",
    "        decompressed = lz4.frame.decompress(input_stream.getvalue())\n",
    "        return baseline_coder.decode(io.BytesIO(decompressed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "class varint_posting_coder:\n",
    "    def encode(output_stream, posting):\n",
    "        unzipped_posting = list(zip(*posting))\n",
    "        varint_coder.encode(output_stream, unzipped_posting[0])\n",
    "        varint_coder.encode(output_stream, unzipped_posting[1])\n",
    "            \n",
    "    def decode(input_stream):\n",
    "        decoded = varint_coder.decode(input_stream)\n",
    "        left = decoded[:len(decoded)//2]\n",
    "        right = decoded[len(decoded)//2:]\n",
    "        return list(zip(left, right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "class delta_coder:\n",
    "    def encode(output_stream, posting):\n",
    "        delta_posting = []\n",
    "        last = -1\n",
    "        for doc_id, f in posting:\n",
    "            if last == -1:\n",
    "                delta_posting.append((doc_id, f))\n",
    "            else:\n",
    "                delta_posting.append((doc_id - last - 1, f))\n",
    "            last = doc_id\n",
    "        lz4_baseline_coder.encode(output_stream, delta_posting)\n",
    "        \n",
    "    def decode(input_stream):\n",
    "        delta_posting = lz4_baseline_coder.decode(input_stream)\n",
    "        posting = []\n",
    "        last = -1\n",
    "        for doc_id, f in delta_posting:\n",
    "            if last == -1:\n",
    "                last = doc_id\n",
    "            else:\n",
    "                last = doc_id + last + 1\n",
    "            posting.append((last, f))\n",
    "\n",
    "        return posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posting2delta(posting):\n",
    "    delta_posting = []\n",
    "    last = -1\n",
    "    for doc_id, f in posting:\n",
    "        if last == -1:\n",
    "            delta_posting.append((doc_id, f))\n",
    "        else:\n",
    "            delta_posting.append((doc_id - last - 1, f))\n",
    "        last = doc_id\n",
    "    return delta_posting\n",
    "\n",
    "\n",
    "def delta2posting(delta_posting):\n",
    "    posting = []\n",
    "    last = -1\n",
    "    for doc_id, f in delta_posting:\n",
    "        if last == -1:\n",
    "            last = doc_id\n",
    "        else:\n",
    "            last = doc_id + last + 1\n",
    "        posting.append((last, f))\n",
    "    return posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "class delta_coder:\n",
    "    def encode(output_stream, posting):\n",
    "        delta_posting = posting2delta(posting)\n",
    "        lz4_baseline_coder.encode(output_stream, delta_posting)\n",
    "        \n",
    "    def decode(input_stream):\n",
    "        delta_posting = lz4_baseline_coder.decode(input_stream)\n",
    "        return delta2posting(delta_posting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "class delta_varint_coder:\n",
    "    def encode(output_stream, posting):\n",
    "        delta_posting = posting2delta(posting)\n",
    "        varint_posting_coder.encode(output_stream, delta_posting)\n",
    "        \n",
    "    def decode(input_stream):\n",
    "        delta_posting = varint_posting_coder.decode(input_stream)\n",
    "        return delta2posting(delta_posting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_encoded_size(coder, index):\n",
    "    total_size = 0\n",
    "    for term, posting in tqdm(index.items()):\n",
    "        output = io.BytesIO()\n",
    "        coder.encode(output, posting)\n",
    "        data = output.getvalue()\n",
    "        total_size += len(data)\n",
    "        decoded_posting = coder.decode(io.BytesIO(data))\n",
    "        assert decoded_posting == posting, f\"{decoded_posting} != {posting}\"\n",
    "    print(f\"{coder.__name__}: {total_size/1024/1024} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 124173/124173 [00:01<00:00, 85254.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline_coder: 9.510276794433594 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_encoded_size(baseline_coder, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 124173/124173 [00:02<00:00, 60467.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lz4_baseline_coder: 8.887649536132812 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_encoded_size(lz4_baseline_coder, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 124173/124173 [00:04<00:00, 27815.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "varint_posting_coder: 3.551640510559082 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_encoded_size(varint_posting_coder, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 124173/124173 [00:02<00:00, 45296.79it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_coder: 8.740694999694824 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_encoded_size(delta_coder, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 124173/124173 [00:04<00:00, 26940.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_varint_coder: 2.8513011932373047 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_encoded_size(delta_varint_coder, index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание 6\n",
    "\n",
    "В данном домашнем задании Вам предстоит реализовать автоматическое исправление опечаток в запросах пользователей. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Датасет\n",
    "Для оценки качества алгоритма исправления опечаток, Вам предоставляется файл `queries.tsv.gz`. В каждой строке файла записаны два запроса – исходный и исправленный. Для простоты, оба запроса будут иметь одинаковое количество слов и отличаться незначительно. Зачастую исходный и исправленный запрос совпадают, что означает что исправлять такой запрос не требуется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/kondratyonok/.pyenv/versions/3.8.5/lib/python3.8/site-packages (4.50.0)\n",
      "Requirement already satisfied: termcolor in /Users/kondratyonok/.pyenv/versions/3.8.5/lib/python3.8/site-packages (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!~/.pyenv/versions/3.8.5/bin/pip3.8 install tqdm termcolor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Generator, Callable\n",
    "\n",
    "Query = str\n",
    "Sentence = str\n",
    "Filename = str\n",
    "Word = str\n",
    "Queries = List[Tuple[Query, Query]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lake compo\u001b[32mu\u001b[0mnd the park\n",
      "traditional c\u001b[31mh\u001b[0m\u001b[32ml\u001b[0mothes\n",
      "\u001b[32mc\u001b[0m\u001b[32ma\u001b[0m\u001b[32mp\u001b[0m\u001b[32mt\u001b[0m\u001b[32ma\u001b[0m\u001b[32mi\u001b[0m\u001b[32mn\u001b[0m\u001b[32m \u001b[0mjack sparrow\n"
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "import difflib\n",
    "\n",
    "def diff_queries(original: Query, fixed: Query) -> Query:\n",
    "    result = ''\n",
    "    for pos, d in enumerate(difflib.ndiff(original, fixed)):\n",
    "        if d[0] == '+':\n",
    "            result += colored(d[2], 'green')\n",
    "        elif d[0] == '-':\n",
    "            result += colored(d[2], 'red')\n",
    "        else:\n",
    "            result += d[2]\n",
    "    return result\n",
    "\n",
    "print(diff_queries(\"lake compond the park\", \"lake compound the park\"))\n",
    "print(diff_queries(\"traditional chothes\", \"traditional clothes\"))\n",
    "print(diff_queries(\"jack sparrow\", \"captain jack sparrow\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 102436 queries\n",
      "\n",
      "emb\u001b[31me\u001b[0m\u001b[32ma\u001b[0mr\u001b[31mi\u001b[0m\u001b[32mr\u001b[0m\u001b[32ma\u001b[0mssing red carpet moments\n",
      "grants for rural areas flo\u001b[32mr\u001b[0mi\u001b[31mr\u001b[0mda\n",
      "the home \u001b[31mh\u001b[0m\u001b[32md\u001b[0mepot merchandising\n",
      "delaware motorcycle inspectio\u001b[32mn\u001b[0m requirements\n",
      "highland park hospital gastric b\u001b[31mi\u001b[0m\u001b[32my\u001b[0mpass surgery\n",
      "grand the\u001b[31mi\u001b[0mft auto\n",
      "windward community college\n",
      "my credit reports\n",
      "st\u001b[32mr\u001b[0mack intermediate school\n",
      "mongol empire political system\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "\n",
    "def load_queries(fn: Filename) -> Queries:\n",
    "    result = []\n",
    "    with gzip.open(fn, 'rt', encoding='utf8') as inp:\n",
    "        for line in inp:\n",
    "            original, fixed = line.rstrip('\\n').split('\\t')\n",
    "            result.append((original, fixed))\n",
    "    return result\n",
    "\n",
    "queries = load_queries(\"./queries.tsv.gz\")\n",
    "print(f'Loaded {len(queries)} queries\\n')\n",
    "for original, fixed in queries[10:20]:\n",
    "    print(diff_queries(original, fixed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_sample = [\n",
    "    (\"grand theift auto\", \"grand theft auto\"),\n",
    "    (\"belarus longitude and latitdue\", \"belarus longitude and latitude\"),\n",
    "    (\"search for poeoms\", \"search for poems\"),\n",
    "    (\"large guacolmoi dip restaurtant price\", \"large guacamole dip restaurant price\"),\n",
    "    (\"texas chainsaw mascurer\", \"texas chainsaw massacre\"),\n",
    "    (\"royal trump subtitle\", \"royal tramp subtitle\"),\n",
    "    (\"florida fiberglass polls\", \"florida fiberglass pools\"),\n",
    "    (\"how to make a calender\", \"how to make a calendar\"),\n",
    "    (\"university of south caroline\", \"university of south carolina\"),\n",
    "    (\"maureen mcdonald in virginia\", \"maureen mcdonnell in virginia\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для составления словаря и обучения языковых моделей Вам предоставляется небольшой корпус текста, неслучайная выборка из большой английской википедии в файле `train.bz2`. Этот файл содержит примерно 5 млн строк или 80 млн слов. Каждая строка – одно предложение без знаков препинания.\n",
    "Использование других словарей и корпусов запрещено."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee3230da7ea84714ab7e1241a51f774f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gol neshin\n",
      "mitochondrial dna depletion syndrome mds or mdds is any of a group of autosomal recessive disorders that cause a significant drop in mitochondrial dna in affected tissues\n",
      "following the relegation of sc freiburg in 2005 he was on the verge of signing for metalurg donetsk but instead he accepted a contract with vfl wolfsburg\n",
      "the first issue for geometers is what kind of geometry is adequate for a novel situation\n",
      "cedar grove was formerly a stage and freight stop\n",
      "regular bus service runs from bhubaneswar to niali which is away\n",
      "later they were also known for the cream wafer biscuits\n",
      "strabomantis cornutus\n",
      "gtk+ scene graph kit gsk was initially released as part of gtk+ 3.90 in march 2017 and is meant for gtk-based applications that wish to replace clutter for their ui\n",
      "the match took place on 10 april 1906 at the hipódromo madrid\n",
      "the brothers came from fresno california\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import bz2\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def read_huge_corpus(fn: Filename) -> Generator[Sentence, None, None]:\n",
    "    with bz2.open(fn, 'rt', encoding='utf8') as inp:\n",
    "        for line in tqdm(inp):\n",
    "            yield line.rstrip('\\n')\n",
    "\n",
    "for li, line in enumerate(read_huge_corpus(\"./train.bz2\")):\n",
    "    print(line)\n",
    "    if li == 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Поиск близких слов\n",
    "Требуется научится быстро находить список из сотни слов, которые незначительно отличаются от заданного слова.\n",
    "\n",
    "Не стоит перебирать все слова словаря – займёт слишком много времени.\n",
    "\n",
    "Для ускорения перебора предлагается создать триграммный индекс – для каждой буквенной триграммы храним список слов, в которых она есть. Тогда для поиска похожих на данное слово найдем слова большим количеством совпадающих триграмм. \n",
    "\n",
    "Совет 1: стоит сделать отельный индекс для каждой длинны слова и использовать только те индексы, в которых лежат слова близкие по длине к исходному.\n",
    "\n",
    "Совет 2: для выделения триграмм стоит обрамить слово спецсимволом, чтобы триграммы на концах слова отличались от оных в середине.\n",
    "\n",
    "Любые другие алгоритмы, улучшающие качество за разумное время (хождение по бору с ошибками, перебор ошибок) – не возбраняются.\n",
    "\n",
    "Не побрезгуйте кешировать результат работы этого алгоритма, чтобы дальнейшая работа протекала быстрее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os.path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "if os.path.exists(\"all_words\"):\n",
    "    all_words = pickle.load(open(\"all_words\", 'rb'))\n",
    "else:\n",
    "    all_words = defaultdict(int)\n",
    "    train = read_huge_corpus(\"./train.bz2\")\n",
    "    for ind, text in enumerate(train):\n",
    "        for word in text.split(\" \"):\n",
    "            all_words[word] += 1\n",
    "    all_words = {i: all_words[i] for i in all_words if all_words[i] > 5}\n",
    "    pickle.dump(all_words, open(\"all_words\", 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "if os.path.exists(\"trigram_index\"):\n",
    "    trigram_index = pickle.load(open(\"trigram_index\", 'rb'))\n",
    "else:\n",
    "    trigram_index = defaultdict(set)\n",
    "\n",
    "    for word in tqdm(all_words):\n",
    "        temp = \"$$\" + word + \"$$\"\n",
    "        for trigram in [temp[i:i+3] for i in range(len(temp)-2)]:\n",
    "            trigram_index[(len(word), trigram)].add(word)\n",
    "    \n",
    "    for l, t in trigram_index:\n",
    "        trigram_index[(l, t)] = list(trigram_index[(l, t)])\n",
    "\n",
    "    pickle.dump(trigram_index, open(\"trigram_index\", 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"words_to_fix\"):\n",
    "    words_to_fix = pickle.load(open(\"words_to_fix\", 'rb'))\n",
    "else:\n",
    "    words_to_fix = []\n",
    "\n",
    "    for query in tqdm(queries):\n",
    "        for original, fixed in zip(query[0].split(\" \"), query[1].split(\" \")):\n",
    "            if original != fixed:\n",
    "                words_to_fix.append((original, fixed))\n",
    "                \n",
    "    pickle.dump(words_to_fix, open(\"words_to_fix\", 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chothes - ok\n",
      "  clothes\n",
      "  choses\n",
      "  chores\n",
      "  chokes\n",
      "  choices\n",
      "\n",
      "cataloges - ok\n",
      "  catalogues\n",
      "  catalogs\n",
      "  cataloged\n",
      "  catalogus\n",
      "  catalog\n",
      "\n",
      "compond - ok\n",
      "  compound\n",
      "  composed\n",
      "  component\n",
      "  compost\n",
      "  commend\n",
      "\n",
      "barns - ok\n",
      "  barns\n",
      "  barnes\n",
      "  bairns\n",
      "  barnas\n",
      "  barons\n",
      "\n",
      "emberissing - ok\n",
      "  embossing\n",
      "  embarrassing\n",
      "  embedding\n",
      "  embezzling\n",
      "  embellishing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "similar_words_cache = {}\n",
    "\n",
    "def find_similar_words(word: Word, len_gap=2, similar_count=1000) -> List[Word]:\n",
    "    if word in similar_words_cache:\n",
    "        return similar_words_cache[word][:similar_count]\n",
    "    temp = \"$$\" + word + \"$$\"\n",
    "    similar = []\n",
    "    for trigram in [temp[i:i+3] for i in range(len(temp)-2)]:\n",
    "        for word_len in range(len(word) - len_gap, len(word) + len_gap + 1):\n",
    "            if (word_len, trigram) in trigram_index:\n",
    "                similar += trigram_index[(word_len, trigram)]\n",
    "    similar = Counter(similar)\n",
    "    similar_words_cache[word] = sorted(list(similar.keys()), key=lambda w:similar[w] / len(word), reverse=True)\n",
    "    return similar_words_cache[word][:similar_count]\n",
    "\n",
    "\n",
    "for original, fixed in words_to_fix[:5]:\n",
    "    similar = find_similar_words(original)\n",
    "    print(original, '- ok' if fixed in similar else '- fail')\n",
    "    for word in similar[:5]:\n",
    "        print(' ', word)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы оценить качество полученного алгоритма, используйте запросы из `queries.tsv.gz`. Отберите только отличающиеся слова в исправленном и исходном запросах. Проверьте, что для слова в исходном запросе, исправленное слово будет в списке ближайших выданном вашим алгоритмом. Если это выполняется для всех или почти всех пар – успех. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 53495 words to fix\n",
      "c\u001b[31mh\u001b[0m\u001b[32ml\u001b[0mothes\n",
      "catalog\u001b[31me\u001b[0ms\n",
      "compo\u001b[32mu\u001b[0mnd\n",
      "barn\u001b[32me\u001b[0ms\n",
      "emb\u001b[31me\u001b[0m\u001b[32ma\u001b[0mr\u001b[31mi\u001b[0m\u001b[32mr\u001b[0m\u001b[32ma\u001b[0mssing\n",
      "flo\u001b[32mr\u001b[0mi\u001b[31mr\u001b[0mda\n",
      "\u001b[31mh\u001b[0m\u001b[32md\u001b[0mepot\n",
      "inspectio\u001b[32mn\u001b[0m\n",
      "b\u001b[31mi\u001b[0m\u001b[32my\u001b[0mpass\n",
      "the\u001b[31mi\u001b[0mft\n"
     ]
    }
   ],
   "source": [
    "def extract_different_words(queries: Queries) -> List[Tuple[Word, Word]]:\n",
    "    words_to_fix = []\n",
    "    for original, fixed in queries:\n",
    "        if original != fixed:\n",
    "            for word_orig, word_fixed in zip(original.split(), fixed.split()):\n",
    "                if word_orig != word_fixed:\n",
    "                    words_to_fix.append((word_orig, word_fixed))\n",
    "    return words_to_fix\n",
    "                    \n",
    "words_to_fix = extract_different_words(queries)\n",
    "print(f'Found {len(words_to_fix)} words to fix')\n",
    "for original, fixed in words_to_fix[:10]:\n",
    "    print(diff_queries(original, fixed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81966c1023ad44c782d033af4341c2c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=53495.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word_to_similar = {}\n",
    "\n",
    "def check_find_similar_words(words_to_fix: List[Tuple[Word, Word]], \n",
    "                             find_similar_words: Callable[[Word], List[Word]], \n",
    "                             debug: bool):\n",
    "    wrong, total = 0, 0\n",
    "    progress = tqdm(words_to_fix)\n",
    "    debug_output = 0\n",
    "    for word_orig, word_fixed in progress:\n",
    "        similar = find_similar_words(word_orig)\n",
    "        if word_fixed not in similar:\n",
    "            wrong += 1\n",
    "            if debug:\n",
    "                print(word_orig, word_fixed)\n",
    "                debug_output += 1\n",
    "                if debug_output == 10:\n",
    "                    break\n",
    "        total += 1\n",
    "        progress.set_description(f'Wrong: {wrong} - {wrong/total*100:0.2f}%')\n",
    "        \n",
    "check_find_similar_words(words_to_fix, find_similar_words, debug=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Языковая модель\n",
    "Языковая модель – модель, которая по тексту оценивает вероятность того, что он мог появиться в языке. \n",
    "\n",
    "Постройте простую n-грамную языковую модель с использованием корпуса текстов `train.bz2`. Для этого рассчитайте количество вхождений каждой n-граммы в корпус текста. Если взять n=2, то размера оперативной памяти вашего компьютера должно будет хватить.\n",
    "\n",
    "Воспользуйтесь каким-нибудь методом сглаживания, чтобы не получать нулевую вероятность для неизвестных n-грамм. Также, чтобы вероятности слов, которых нет в словаре, были отличны от нуля, можно примешать побуквенную m-граммную модель.\n",
    "\n",
    "Совет N: если количество оперативной памяти прижмёт, можно хранить строки в виде байт – один раскодированный символ занимает больше памяти чем один байт, при этом для английского текста почти всегда один символ кодируется одним байтом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "\n",
    "# word n-gram\n",
    "if os.path.exists(\"n_grams\"):\n",
    "    n_grams = pickle.load(open(\"n_grams\", 'rb'))\n",
    "else:\n",
    "    n_grams = {}\n",
    "    train = read_huge_corpus(\"./train.bz2\")\n",
    "    for ind, text in enumerate(train):\n",
    "        words = text.split(\" \")\n",
    "        for n_gram in [\" \".join(words[i:i+n]) for i in range(len(words)-n+1)]:\n",
    "            if n_gram not in n_grams:\n",
    "                n_grams[n_gram] = 0\n",
    "            n_grams[n_gram] += 1\n",
    "    pickle.dump(n_grams, open(\"n_grams\", 'wb'))\n",
    "\n",
    "total_n_grams = len(n_grams) + sum(n_grams[n_gram] for n_gram in n_grams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 3\n",
    "\n",
    "# char m-gram\n",
    "if os.path.exists(\"m_grams\"):\n",
    "    m_grams = pickle.load(open(\"m_grams\", 'rb'))\n",
    "else:\n",
    "    m_grams = {}\n",
    "    train = read_huge_corpus(\"./train.bz2\")\n",
    "    for ind, text in enumerate(train):\n",
    "        for word in text.split(\" \"):\n",
    "            temp = \"$\" + word + \"$\"\n",
    "            for m_gram in [temp[i:i+m] for i in range(len(temp)-m+1)]:\n",
    "                if m_gram not in m_grams:\n",
    "                    m_grams[m_gram] = 0\n",
    "                m_grams[m_gram] += 1\n",
    "    pickle.dump(m_grams, open(\"m_grams\", 'wb'))\n",
    "\n",
    "total_m_grams = len(m_grams) + sum(m_grams[m_gram] for m_gram in m_grams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok]                          grand theift auto -141.77  <   0.00 grand theft auto\n",
      "[ok]             belarus longitude and latitdue -118.81  <  -4.00 belarus longitude and latitude\n",
      "[ok]                          search for poeoms -90.85  <   0.00 search for poems\n",
      "[ok]      large guacolmoi dip restaurtant price -521.54  <  -130.66 large guacamole dip restaurant price\n",
      "[ok]                    texas chainsaw mascurer -96.22  <   0.00 texas chainsaw massacre\n",
      "[ok]                       royal trump subtitle -6.00  <  -3.00 royal tramp subtitle\n",
      "[fail]                 florida fiberglass polls -6.00  >= -6.00 florida fiberglass pools\n",
      "[ok]                     how to make a calender -85.75  <   0.00 how to make a calendar\n",
      "[ok]               university of south caroline -4.00  <   0.00 university of south carolina\n",
      "[fail]             maureen mcdonald in virginia  0.00  >= -4.00 maureen mcdonnell in virginia\n"
     ]
    }
   ],
   "source": [
    "from math import log2\n",
    "\n",
    "total_words = len(all_words) + sum(all_words[word] for word in all_words)\n",
    "\n",
    "def get_probability(query: Query) -> float: # log probability\n",
    "    probability = 0\n",
    "    words = query.split(\" \")\n",
    "    for n_gram in [\" \".join(words[i:i+n]) for i in range(len(words)-n+1)]:\n",
    "        if n_gram in n_grams:\n",
    "            probability += 0#log2((n_grams[n_gram] + 1) / total_n_grams)\n",
    "        else:\n",
    "            probability += -1 * len(words)\n",
    "            for word in n_gram.split(\" \"):\n",
    "                if word in all_words:\n",
    "                    probability += 0#log2((all_words[word] + 1) / total_words)\n",
    "                else:\n",
    "                    probability += -1\n",
    "                    temp = \"$\" + word + \"$\"\n",
    "                    for m_gram in [temp[i:i+m] for i in range(len(temp)-m+1)]:\n",
    "                        if m_gram in m_grams:\n",
    "                            probability += log2((m_grams[m_gram] + 1) / total_m_grams)\n",
    "    return probability\n",
    "\n",
    "for original, fixed in queries_sample:\n",
    "    p_original = get_probability(original)\n",
    "    p_fixed = get_probability(fixed)\n",
    "    verdict = '[ok]  ' if p_fixed > p_original else '[fail]'\n",
    "    sign = '< ' if p_fixed > p_original else '>='\n",
    "    print(f'{verdict} {original:>40s} {p_original:5.2f}  {sign} {p_fixed:5.2f} {fixed}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы оценить качество полученной модели, используйте запросы из `queries.tsv.gz`. Сравните вероятность, которую выдает ваша модель для исходных и исправленных запросов. Хорошая модель выдаёт исправленному запросу большую вероятность. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb66b4650744e47892b871e826affa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=102436.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def check_language_model(queries: Queries, get_probability: Callable[[Query], float], debug: bool):\n",
    "    wrong, total = 0, 0\n",
    "    progress = tqdm(queries)\n",
    "    debug_output = 0\n",
    "    for original, fixed in progress:\n",
    "        if original == fixed:\n",
    "            continue\n",
    "        p_original = get_probability(original)\n",
    "        p_fixed = get_probability(fixed)\n",
    "        if p_fixed <= p_original:\n",
    "            wrong += 1\n",
    "            if debug:\n",
    "                print(original, p_original)\n",
    "                print(fixed, p_fixed)\n",
    "                print()\n",
    "                debug_output += 1\n",
    "                if debug_output == 10:\n",
    "                    break\n",
    "        total += 1\n",
    "        progress.set_description(f'Wrong: {wrong} - {wrong/total*100:0.2f}%')\n",
    "        \n",
    "check_language_model(queries, get_probability, debug=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Советую сохранить полученную модель на диск – а случае чего, чтение статистик с диска, может быть быстрее расчёта оных с нуля."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Модель ошибок\n",
    "Модель ошибок – модель которая по исходному и исправленному запросу оценивает вероятность того, что такая ошибка могла быть допущена.\n",
    "\n",
    "Рассчитайте простую модель ошибок на основе расстояния Дамерау-Левенштейна, то есть модифицированного Левенштейна, который считает перестановку соседних букв за одну ошибку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def damerau_levenshtein_distance(s1, s2):\n",
    "    d = {}\n",
    "    lenstr1 = len(s1)\n",
    "    lenstr2 = len(s2)\n",
    "    for i in range(-1,lenstr1+1):\n",
    "        d[(i,-1)] = i+1\n",
    "    for j in range(-1,lenstr2+1):\n",
    "        d[(-1,j)] = j+1\n",
    "\n",
    "    for i in range(lenstr1):\n",
    "        for j in range(lenstr2):\n",
    "            if s1[i] == s2[j]:\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = 1\n",
    "            d[(i,j)] = min(\n",
    "                           d[(i-1,j)] + 1, # deletion\n",
    "                           d[(i,j-1)] + 1, # insertion\n",
    "                           d[(i-1,j-1)] + cost, # substitution\n",
    "                          )\n",
    "            if i and j and s1[i]==s2[j-1] and s1[i-1] == s2[j]:\n",
    "                d[(i,j)] = min (d[(i,j)], d[i-2,j-2] + cost) # transposition\n",
    "\n",
    "    return d[lenstr1-1,lenstr2-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       grand theift auto | -0.09 | grand theft auto\n",
      "          belarus longitude and latitdue | -0.05 | belarus longitude and latitude\n",
      "                       search for poeoms | -0.09 | search for poems\n",
      "   large guacolmoi dip restaurtant price | -0.21 | large guacamole dip restaurant price\n",
      "                 texas chainsaw mascurer | -0.28 | texas chainsaw massacre\n",
      "                    royal trump subtitle | -0.07 | royal tramp subtitle\n",
      "                florida fiberglass polls | -0.06 | florida fiberglass pools\n",
      "                  how to make a calender | -0.07 | how to make a calendar\n",
      "            university of south caroline | -0.05 | university of south carolina\n",
      "            maureen mcdonald in virginia | -0.16 | maureen mcdonnell in virginia\n"
     ]
    }
   ],
   "source": [
    "from math import log2\n",
    "\n",
    "def get_error_probability(original: Query, fixed: Query) -> float:\n",
    "    dist = damerau_levenshtein_distance(original, fixed)\n",
    "    avg_len = (len(original) + len(fixed)) / 2.\n",
    "    if avg_len - dist < 0:\n",
    "        return -1 * 10**10\n",
    "    return log2((avg_len - dist + 0.00001) / avg_len)\n",
    "\n",
    "for original, fixed in queries_sample:\n",
    "    p_error = get_error_probability(original, fixed)\n",
    "    print(f'{original:>40s} | {p_error:5.2f} | {fixed}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Олтугеза\n",
    "Объедините результат работы предыдущих пунктов в единый алгоритм исправления опечатки для запроса.\n",
    "\n",
    "Примерный план:\n",
    "1.\tДля слов запроса генерируем список ближайших слов-кандидатов (для всех, даже словарных слов).\n",
    "2.\tСобираем список кандидатов-запросов (эвристически, чтобы не сделать экспоненциальное время выполнения)\n",
    "3.\tДля каждого кандидата считаем итоговый объединенный score на основе языковой модели и модели ошибок для данного кандидата (не обязательно сумма или произведение, можно объединение любой сложности).\n",
    "4.\tВыдаём гипотезу с наибольшим score.\n",
    "5.\t???\n",
    "6.\tProfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok]                           grand theft auto == grand theft auto\n",
      "[ok]             belarus longitude and latitude == belarus longitude and latitude\n",
      "[ok]                           search for poems == search for poems\n",
      "[fail]       large giacomo dip restaurant price != large guacamole dip restaurant price\n",
      "[fail]                    texas chainsaw maurer != texas chainsaw massacre\n",
      "[fail]                     royal trump subtitle != royal tramp subtitle\n",
      "[fail]                 florida fiberglass poles != florida fiberglass pools\n",
      "[ok]                     how to make a calendar == how to make a calendar\n",
      "[fail]             university of south caroline != university of south carolina\n",
      "[fail]             maureen mcdonald in virginia != maureen mcdonnell in virginia\n"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "from itertools import product\n",
    "\n",
    "def correct(query: Query) -> Query:\n",
    "    queries = []\n",
    "    similar = {}\n",
    "    for word in query.split(\" \"):\n",
    "        similar[word] = sorted(\n",
    "            find_similar_words(word),\n",
    "            key=lambda x: get_error_probability(word, x),\n",
    "            reverse=True\n",
    "        )[:2]\n",
    "    similar_queries = sorted(\n",
    "        [\" \".join(pr) for pr in product(*[similar[word] for word in query.split(\" \")])],\n",
    "        key=lambda x: get_probability(x),\n",
    "        reverse=True\n",
    "    )\n",
    "    return similar_queries[0]\n",
    "\n",
    "for original, fixed in queries_sample:\n",
    "    predict = correct(original)\n",
    "    verdict = '[ok]  ' if predict == fixed else '[fail]'\n",
    "    sign = '==' if predict == fixed else '!='\n",
    "    print(f'{verdict} {predict:>40s} {sign} {fixed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итоговое качество меряем на примерах из `queries.tsv.gz`.\n",
    "\n",
    "Для отладки проблем с качеством имеет смысл научится понимать на каком этапе теряется правильная гипотеза для каждого примера. Например, если правильное исправление есть в списке кандидатов (п. 2), но не выбирается как лучшая – стоит крутить языковую модель, модель ошибок и их объединение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927687f97bcd43d4a550b51196a87d6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1025.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def check_corrector(queries: Queries, correct: Callable[[Query], Query], debug: bool):\n",
    "    wrong, total = 0, 0\n",
    "    progress = tqdm(queries[::100])\n",
    "    debug_output = 0\n",
    "    for original, fixed in progress:\n",
    "        predict = correct(original)\n",
    "        if predict != fixed:\n",
    "            wrong += 1\n",
    "            if debug:\n",
    "                print(original)\n",
    "                print(fixed)\n",
    "                print(predict)\n",
    "                print()\n",
    "                debug_output += 1\n",
    "                if debug_output == 10:\n",
    "                    break\n",
    "        total += 1\n",
    "        progress.set_description(f'Wrong: {wrong} - {wrong/total*100:0.2f}%')\n",
    "        \n",
    "check_corrector(queries, correct, debug=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все запросы проходят как-то очень долго. На половине запросов было стабильно 25.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

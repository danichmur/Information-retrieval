{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание 6\n",
    "\n",
    "В данном домашнем задании Вам предстоит реализовать автоматическое исправление опечаток в запросах пользователей. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Датасет\n",
    "Для оценки качества алгоритма исправления опечаток, Вам предоставляется файл `queries.tsv.gz`. В каждой строке файла записаны два запроса – исходный и исправленный. Для простоты, оба запроса будут иметь одинаковое количество слов и отличаться незначительно. Зачастую исходный и исправленный запрос совпадают, что означает что исправлять такой запрос не требуется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Generator, Callable\n",
    "\n",
    "Query = str\n",
    "Sentence = str\n",
    "Filename = str\n",
    "Word = str\n",
    "Queries = List[Tuple[Query, Query]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lake compo\u001b[32mu\u001b[0mnd the park\n",
      "traditional c\u001b[31mh\u001b[0m\u001b[32ml\u001b[0mothes\n",
      "\u001b[32mc\u001b[0m\u001b[32ma\u001b[0m\u001b[32mp\u001b[0m\u001b[32mt\u001b[0m\u001b[32ma\u001b[0m\u001b[32mi\u001b[0m\u001b[32mn\u001b[0m\u001b[32m \u001b[0mjack sparrow\n"
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "import difflib\n",
    "import pickle\n",
    "\n",
    "def diff_queries(original: Query, fixed: Query) -> Query:\n",
    "    result = ''\n",
    "    for pos, d in enumerate(difflib.ndiff(original, fixed)):\n",
    "        if d[0] == '+':\n",
    "            result += colored(d[2], 'green')\n",
    "        elif d[0] == '-':\n",
    "            result += colored(d[2], 'red')\n",
    "        else:\n",
    "            result += d[2]\n",
    "    return result\n",
    "\n",
    "print(diff_queries(\"lake compond the park\", \"lake compound the park\"))\n",
    "print(diff_queries(\"traditional chothes\", \"traditional clothes\"))\n",
    "print(diff_queries(\"jack sparrow\", \"captain jack sparrow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 102436 queries\n",
      "\n",
      "emb\u001b[31me\u001b[0m\u001b[32ma\u001b[0mr\u001b[31mi\u001b[0m\u001b[32mr\u001b[0m\u001b[32ma\u001b[0mssing red carpet moments\n",
      "grants for rural areas flo\u001b[32mr\u001b[0mi\u001b[31mr\u001b[0mda\n",
      "the home \u001b[31mh\u001b[0m\u001b[32md\u001b[0mepot merchandising\n",
      "delaware motorcycle inspectio\u001b[32mn\u001b[0m requirements\n",
      "highland park hospital gastric b\u001b[31mi\u001b[0m\u001b[32my\u001b[0mpass surgery\n",
      "grand the\u001b[31mi\u001b[0mft auto\n",
      "windward community college\n",
      "my credit reports\n",
      "st\u001b[32mr\u001b[0mack intermediate school\n",
      "mongol empire political system\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "\n",
    "def load_queries(fn: Filename) -> Queries:\n",
    "    result = []\n",
    "    with gzip.open(fn, 'rt', encoding='utf8') as inp:\n",
    "        for line in inp:\n",
    "            original, fixed = line.rstrip('\\n').split('\\t')\n",
    "            result.append((original, fixed))\n",
    "    return result\n",
    "\n",
    "queries = load_queries(\"./queries.tsv.gz\")\n",
    "print(f'Loaded {len(queries)} queries\\n')\n",
    "for original, fixed in queries[10:20]:\n",
    "    print(diff_queries(original, fixed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_sample = [\n",
    "    (\"grand theift auto\", \"grand theft auto\"),\n",
    "    (\"belarus longitude and latitdue\", \"belarus longitude and latitude\"),\n",
    "    (\"search for poeoms\", \"search for poems\"),\n",
    "    (\"large guacolmoi dip restaurtant price\", \"large guacamole dip restaurant price\"),\n",
    "    (\"texas chainsaw mascurer\", \"texas chainsaw massacre\"),\n",
    "    (\"royal trump subtitle\", \"royal tramp subtitle\"),\n",
    "    (\"florida fiberglass polls\", \"florida fiberglass pools\"),\n",
    "    (\"how to make a calender\", \"how to make a calendar\"),\n",
    "    (\"university of south caroline\", \"university of south carolina\"),\n",
    "    (\"maureen mcdonald in virginia\", \"maureen mcdonnell in virginia\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для составления словаря и обучения языковых моделей Вам предоставляется небольшой корпус текста, неслучайная выборка из большой английской википедии в файле `train.bz2`. Этот файл содержит примерно 5 млн строк или 80 млн слов. Каждая строка – одно предложение без знаков препинания.\n",
    "Использование других словарей и корпусов запрещено."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 326.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 gol neshin\n",
      "1 mitochondrial dna depletion syndrome mds or mdds is any of a group of autosomal recessive disorders that cause a significant drop in mitochondrial dna in affected tissues\n",
      "2 following the relegation of sc freiburg in 2005 he was on the verge of signing for metalurg donetsk but instead he accepted a contract with vfl wolfsburg\n",
      "3 the first issue for geometers is what kind of geometry is adequate for a novel situation\n",
      "4 cedar grove was formerly a stage and freight stop\n",
      "5 regular bus service runs from bhubaneswar to niali which is away\n",
      "6 later they were also known for the cream wafer biscuits\n",
      "7 strabomantis cornutus\n",
      "8 gtk+ scene graph kit gsk was initially released as part of gtk+ 3.90 in march 2017 and is meant for gtk-based applications that wish to replace clutter for their ui\n",
      "9 the match took place on 10 april 1906 at the hipódromo madrid\n",
      "10 the brothers came from fresno california\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import bz2\n",
    "from tqdm import tqdm\n",
    "\n",
    "def read_huge_corpus(fn: Filename) -> Generator[Sentence, None, None]:\n",
    "    with bz2.open(fn, 'rt', encoding='utf8') as inp:\n",
    "        for line in tqdm(inp):\n",
    "            yield line.rstrip('\\n')\n",
    "\n",
    "for li, line in enumerate(read_huge_corpus(\"./train.bz2\")):\n",
    "    print(li, line)\n",
    "    if li == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Поиск близких слов\n",
    "Требуется научится быстро находить список из сотни слов, которые незначительно отличаются от заданного слова.\n",
    "\n",
    "Не стоит перебирать все слова словаря – займёт слишком много времени.\n",
    "\n",
    "Для ускорения перебора предлагается создать триграммный индекс – для каждой буквенной триграммы храним список слов, в которых она есть. Тогда для поиска похожих на данное слово найдем слова большим количеством совпадающих триграмм. \n",
    "\n",
    "Совет 1: стоит сделать отельный индекс для каждой длинны слова и использовать только те индексы, в которых лежат слова близкие по длине к исходному.\n",
    "\n",
    "Совет 2: для выделения триграмм стоит обрамить слово спецсимволом, чтобы триграммы на концах слова отличались от оных в середине.\n",
    "\n",
    "Любые другие алгоритмы, улучшающие качество за разумное время (хождение по бору с ошибками, перебор ошибок) – не возбраняются.\n",
    "\n",
    "Не побрезгуйте кешировать результат работы этого алгоритма, чтобы дальнейшая работа протекала быстрее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4717753it [01:17, 61144.88it/s]\n"
     ]
    }
   ],
   "source": [
    "id2word = {}\n",
    "id2count = {}\n",
    "word2id = {}\n",
    "\n",
    "last_id = 0\n",
    "\n",
    "for line in read_huge_corpus(\"./train.bz2\"):\n",
    "    for word in line.split():\n",
    "        if word not in word2id:\n",
    "            id2word[last_id] = word\n",
    "            word2id[word] = last_id\n",
    "            last_id += 1\n",
    "        else:\n",
    "            id_ = word2id[word]\n",
    "            cur_count = id2count.get(id_, 1)\n",
    "            id2count[id_] = cur_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('id2word.p', 'rb') as f:\n",
    "    id2word = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4717753it [00:39, 120077.20it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for line in read_huge_corpus(\"./train.bz2\"):\n",
    "    corpus += line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter([k for k in corpus])\n",
    "c = {k:v for k, v in c.items() if v > 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "def frame_word(v):\n",
    "    return f\"<<{v}>>\"\n",
    "\n",
    "def build_indexes(c, n=3):\n",
    "    indexes = {}\n",
    "    for v in tqdm(c.keys()):\n",
    "        length_index = indexes.get(len(v), {})\n",
    "        for ngram in ngrams(frame_word(v), n):\n",
    "            str_ngram = ''.join(ngram)\n",
    "            ngram_set = length_index.get(str_ngram, [])\n",
    "            ngram_set.append(v)\n",
    "            length_index[str_ngram] = ngram_set\n",
    "        indexes[len(v)] = length_index\n",
    "    \n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 435212/435212 [00:06<00:00, 72368.74it/s] \n"
     ]
    }
   ],
   "source": [
    "my_beautiful_indexes = build_indexes(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"id2count.p\", 'wb') as f:\n",
    "    pickle.dump(id2count, f)\n",
    "with open(\"id2word.p\", 'wb') as f:\n",
    "    pickle.dump(id2word, f)\n",
    "with open(\"word2id.p\", 'wb') as f:\n",
    "    pickle.dump(word2id, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы оценить качество полученного алгоритма, используйте запросы из `queries.tsv.gz`. Отберите только отличающиеся слова в исправленном и исходном запросах. Проверьте, что для слова в исходном запросе, исправленное слово будет в списке ближайших выданном вашим алгоритмом. Если это выполняется для всех или почти всех пар – успех. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 53495 words to fix\n",
      "c\u001b[31mh\u001b[0m\u001b[32ml\u001b[0mothes\n",
      "catalog\u001b[31me\u001b[0ms\n",
      "compo\u001b[32mu\u001b[0mnd\n",
      "barn\u001b[32me\u001b[0ms\n",
      "emb\u001b[31me\u001b[0m\u001b[32ma\u001b[0mr\u001b[31mi\u001b[0m\u001b[32mr\u001b[0m\u001b[32ma\u001b[0mssing\n",
      "flo\u001b[32mr\u001b[0mi\u001b[31mr\u001b[0mda\n",
      "\u001b[31mh\u001b[0m\u001b[32md\u001b[0mepot\n",
      "inspectio\u001b[32mn\u001b[0m\n",
      "b\u001b[31mi\u001b[0m\u001b[32my\u001b[0mpass\n",
      "the\u001b[31mi\u001b[0mft\n"
     ]
    }
   ],
   "source": [
    "def extract_different_words(queries: Queries) -> List[Tuple[Word, Word]]:\n",
    "    words_to_fix = []\n",
    "    for original, fixed in queries:\n",
    "        if original != fixed:\n",
    "            for word_orig, word_fixed in zip(original.split(), fixed.split()):\n",
    "                if word_orig != word_fixed:\n",
    "                    words_to_fix.append((word_orig, word_fixed))\n",
    "    return words_to_fix\n",
    "                    \n",
    "words_to_fix = extract_different_words(queries)\n",
    "print(f'Found {len(words_to_fix)} words to fix')\n",
    "for original, fixed in words_to_fix[:10]:\n",
    "    print(diff_queries(original, fixed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def find_similar_words(word: Word, n=3) -> List[Word]:\n",
    "    length = len(word)\n",
    "    word = frame_word(word)\n",
    "    ngrms = list(ngrams(word, n))\n",
    "    uniq_ngrms = set(ngrms)\n",
    "\n",
    "    len_shifts = 2\n",
    "    c = Counter({})\n",
    "    for i in range(length - len_shifts, length + len_shifts + 1):\n",
    "        if i > 0:\n",
    "            index = my_beautiful_indexes.get(i, {})\n",
    "            for ngram in ngrms:\n",
    "                c += Counter(index.get(''.join(ngram), []))\n",
    "                \n",
    "    arr = [(w, count) for w, count in c.items() if count > 2]\n",
    "                   \n",
    "    sorted_arr = sorted(arr, key=lambda x: -x[1])\n",
    "    return [w for w, _ in sorted_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chothes - ok\n",
      "  clothes\n",
      "  chores\n",
      "  chomes\n",
      "  choses\n",
      "  chokes\n",
      "\n",
      "cataloges - ok\n",
      "  catalogues\n",
      "  catalogers\n",
      "  catalogs\n",
      "  cataloged\n",
      "  catalogus\n",
      "\n",
      "compond - ok\n",
      "  compound\n",
      "  composed\n",
      "  component\n",
      "  composted\n",
      "  comarmond\n",
      "\n",
      "barns - ok\n",
      "  barns\n",
      "  barnens\n",
      "  barnes\n",
      "  barons\n",
      "  barnas\n",
      "\n",
      "emberissing - ok\n",
      "  embossing\n",
      "  embarrassing\n",
      "  embedding\n",
      "  embezzling\n",
      "  remembering\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for original, fixed in words_to_fix[:5]:\n",
    "    similar = find_similar_words(original)[:5]\n",
    "   \n",
    "    print(original,'- ok' if fixed in similar else '- fail')\n",
    "    for word in similar[:5]:\n",
    "        print(' ', word)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wrong: 4275 - 7.99%: 100%|██████████| 53495/53495 [50:10<00:00, 17.77it/s]   \n"
     ]
    }
   ],
   "source": [
    "def check_find_similar_words(words_to_fix: List[Tuple[Word, Word]], \n",
    "                             find_similar_words: Callable[[Word], List[Word]], \n",
    "                             debug: bool):\n",
    "    wrong, total = 0, 0\n",
    "    progress = tqdm(words_to_fix)\n",
    "    debug_output = 0\n",
    "    for word_orig, word_fixed in progress:\n",
    "        similar = find_similar_words(word_orig)\n",
    "        if word_fixed not in similar:\n",
    "            wrong += 1\n",
    "            if debug:\n",
    "                print(word_orig, word_fixed)\n",
    "                debug_output += 1\n",
    "                if debuge_output == 10:\n",
    "                    break\n",
    "        total += 1\n",
    "        progress.set_description(f'Wrong: {wrong} - {wrong/total*100:0.2f}%')\n",
    "        \n",
    "check_find_similar_words(words_to_fix, find_similar_words, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Языковая модель\n",
    "Языковая модель – модель, которая по тексту оценивает вероятность того, что он мог появиться в языке. \n",
    "\n",
    "Постройте простую n-грамную языковую модель с использованием корпуса текстов `train.bz2`. Для этого рассчитайте количество вхождений каждой n-граммы в корпус текста. Если взять n=2, то размера оперативной памяти вашего компьютера должно будет хватить.\n",
    "\n",
    "Воспользуйтесь каким-нибудь методом сглаживания, чтобы не получать нулевую вероятность для неизвестных n-грамм. Также, чтобы вероятности слов, которых нет в словаре, были отличны от нуля, можно примешать побуквенную m-граммную модель.\n",
    "\n",
    "Совет N: если количество оперативной памяти прижмёт, можно хранить строки в виде байт – один раскодированный символ занимает больше памяти чем один байт, при этом для английского текста почти всегда один символ кодируется одним байтом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigrams(sentences):\n",
    "    unigrams={}\n",
    "    for sentence in tqdm(sentences):\n",
    "        tokens=sentence.split()\n",
    "        for k in tokens:\n",
    "            if k in c:\n",
    "                if k in unigrams:\n",
    "                    unigrams[k]+=1\n",
    "                else:\n",
    "                    unigrams[k]=1\n",
    "    return unigrams\n",
    "\n",
    "def get_bigrams(sentences):\n",
    "    bigrams={}\n",
    "    for sentence in tqdm(sentences):\n",
    "        tokens=sentence.split()\n",
    "        i=0\n",
    "        length=len(tokens)-1\n",
    "        while i<length:\n",
    "            if tokens[i] not in bigrams:\n",
    "                bigrams[tokens[i]]={}\n",
    "            if tokens[i+1] not in bigrams[tokens[i]]:\n",
    "                bigrams[tokens[i]][tokens[i+1]]=1\n",
    "            else:\n",
    "                bigrams[tokens[i]][tokens[i+1]]+=1\n",
    "            i += 1\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4717753it [00:26, 176770.01it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = [l for l in read_huge_corpus(\"./train.bz2\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4717753/4717753 [00:40<00:00, 117517.67it/s]\n"
     ]
    }
   ],
   "source": [
    "unigrams = get_unigrams(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4717753/4717753 [01:29<00:00, 52781.97it/s]\n"
     ]
    }
   ],
   "source": [
    "bigrams = get_bigrams(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump = True\n",
    "\n",
    "if dump:\n",
    "    with open(\"bigrams.p\", 'wb') as f:\n",
    "        pickle.dump(bigrams, f)\n",
    "    with open(\"unigrams.p\", 'wb') as f:\n",
    "        pickle.dump(unigrams, f)\n",
    "else:\n",
    "    with open('bigrams.p', 'rb') as f:\n",
    "        bigrams = pickle.load(f)\n",
    "    with open('unigrams.p', 'rb') as f:\n",
    "        unigrams = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_probability_1(sentence: Query) -> float:\n",
    "    sentence = frame_query(sentence).split()\n",
    "    score = 0.0\n",
    "    for i in range(len(sentence) - 1):\n",
    "        bigrams_count = bigrams.get(sentence[i], {}).get(sentence[i + 1], 0)\n",
    "        if bigrams_count > 0:\n",
    "            score += math.log(bigrams_count)\n",
    "            score -= math.log(unigrams.get(sentence[i], 1))\n",
    "        else:\n",
    "            score += (math.log(unigrams.get(sentence[i + 1], 1) + 1) + math.log(0.4))\n",
    "            score -= math.log(len(unigrams))\n",
    "    return -1. / score\n",
    "\n",
    "\n",
    "def get_probability_2(sentence: Query) -> float:\n",
    "    sentence = sentence.split()\n",
    "    score = unigrams.get(sentence[0], 0) / W\n",
    "    lam = 0.79\n",
    "    for i in range(len(sentence) - 2):\n",
    "        bigrams_count = bigrams.get(sentence[i], {}).get(sentence[i + 1], 0)\n",
    "        p2g = bigrams_count / unigrams.get(sentence[i], 1)\n",
    "        #print(bigrams_count, unigrams.get(sentence[i], 1))\n",
    "\n",
    "        pw = unigrams.get(sentence[i], 0) / W\n",
    "        score *= (p2g * lam + (1 - lam) * pw)\n",
    "        \n",
    "    return score\n",
    "        \n",
    "    \n",
    "    \n",
    "def get_probability_3(sentence: Query) -> float:\n",
    "    l = [0.6, 0.3, 0.1]\n",
    "    sentence = frame_query(sentence).split()\n",
    "    score = 0.\n",
    "    for i in range(len(sentence) - 2):\n",
    "        trigrams_count = trigrams.get(sentence[i], {}).get(sentence[i + 1], {}).get(sentence[i+2], 0) / tri_W\n",
    "        bigrams_count = bigrams.get(sentence[i + 1], {}).get(sentence[i + 2], 0) / bi_W\n",
    "        unigrams_count = unigrams.get(sentence[i + 2], 0) / W\n",
    "        #print(sentence[i], sentence[i + 1], sentence[i+2])\n",
    "        cur_score = trigrams_count * l[0] + bigrams_count * l[1] + unigrams_count * l[2]\n",
    "        #print(cur_score)#, trigrams_count, bigrams_count, unigrams_count)\n",
    "        score = cur_score\n",
    "        \n",
    "    return score\n",
    "\n",
    "\n",
    "def get_probability(sentence: Query) -> float:\n",
    "    sentence = sentence.split()\n",
    "    score = 0.\n",
    "    for i in range(len(sentence) - 1):\n",
    "        bigrams_count = bigrams.get(sentence[i], {}).get(sentence[i + 1], 0)\n",
    "        if bigrams_count > 0:\n",
    "            score += 1\n",
    "        else:\n",
    "            score -= 1\n",
    "            for w in [sentence[i], sentence[i + 1]]:\n",
    "                if w not in c:\n",
    "                    score -= 1\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok]                          grand theift auto -4.00  <   2.00 grand theft auto\n",
      "[ok]             belarus longitude and latitdue -2.00  <   1.00 belarus longitude and latitude\n",
      "[ok]                          search for poeoms -1.00  <   2.00 search for poems\n",
      "[ok]      large guacolmoi dip restaurtant price -8.00  <  -2.00 large guacamole dip restaurant price\n",
      "[ok]                    texas chainsaw mascurer -1.00  <   2.00 texas chainsaw massacre\n",
      "[ok]                       royal trump subtitle -2.00  <   0.00 royal tramp subtitle\n",
      "[fail]                 florida fiberglass polls -2.00  >= -2.00 florida fiberglass pools\n",
      "[ok]                     how to make a calender  1.00  <   4.00 how to make a calendar\n",
      "[ok]               university of south caroline  1.00  <   3.00 university of south carolina\n",
      "[fail]             maureen mcdonald in virginia  3.00  >=  1.00 maureen mcdonnell in virginia\n"
     ]
    }
   ],
   "source": [
    "for original, fixed in queries_sample:\n",
    "    p_original = get_probability(original)\n",
    "    p_fixed = get_probability(fixed)\n",
    "    verdict = '[ok]  ' if p_fixed > p_original else '[fail]'\n",
    "    sign = '< ' if p_fixed > p_original else '>='\n",
    "    print(f'{verdict} {original:>40s} {p_original:5.2f}  {sign} {p_fixed:5.2f} {fixed}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы оценить качество полученной модели, используйте запросы из `queries.tsv.gz`. Сравните вероятность, которую выдает ваша модель для исходных и исправленных запросов. Хорошая модель выдаёт исправленному запросу большую вероятность. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wrong: 4741 - 9.23%: 100%|██████████| 102436/102436 [00:44<00:00, 2298.68it/s]\n"
     ]
    }
   ],
   "source": [
    "def check_language_model(queries: Queries, get_probability: Callable[[Query], float], debug: bool):\n",
    "    wrong, total = 0, 0\n",
    "    progress = tqdm(queries)\n",
    "    debug_output = 0\n",
    "    for original, fixed in progress:\n",
    "        if original == fixed:\n",
    "            continue\n",
    "        p_original = get_probability(original)\n",
    "        p_fixed = get_probability(fixed)\n",
    "        if p_fixed <= p_original:\n",
    "            wrong += 1\n",
    "            if debug:\n",
    "                print(original, p_original)\n",
    "                print(fixed, p_fixed)\n",
    "                print()\n",
    "                debug_output += 1\n",
    "                if debug_output == 10:\n",
    "                    break\n",
    "        total += 1\n",
    "        progress.set_description(f'Wrong: {wrong} - {wrong/total*100:0.2f}%')\n",
    "        \n",
    "check_language_model(queries, get_probability, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Советую сохранить полученную модель на диск – а случае чего, чтение статистик с диска, может быть быстрее расчёта оных с нуля."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Модель ошибок\n",
    "Модель ошибок – модель которая по исходному и исправленному запросу оценивает вероятность того, что такая ошибка могла быть допущена.\n",
    "\n",
    "Рассчитайте простую модель ошибок на основе расстояния Дамерау-Левенштейна, то есть модифицированного Левенштейна, который считает перестановку соседних букв за одну ошибку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       grand theift auto |  0.67 | grand theft auto\n",
      "          belarus longitude and latitdue |  0.67 | belarus longitude and latitude\n",
      "                       search for poeoms |  0.67 | search for poems\n",
      "   large guacolmoi dip restaurtant price |  0.13 | large guacamole dip restaurant price\n",
      "                 texas chainsaw mascurer |  0.20 | texas chainsaw massacre\n",
      "                    royal trump subtitle |  0.67 | royal tramp subtitle\n",
      "                florida fiberglass polls |  0.67 | florida fiberglass pools\n",
      "                  how to make a calender |  0.67 | how to make a calendar\n",
      "            university of south caroline |  0.67 | university of south carolina\n",
      "            maureen mcdonald in virginia |  0.30 | maureen mcdonnell in virginia\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from math import log2\n",
    "\n",
    "def lev(original: Word, fixed: Word) -> int:\n",
    "\n",
    "    if not original:\n",
    "        return len(fixed)\n",
    "    if not fixed:\n",
    "        return len(original)\n",
    "    \n",
    "    lenstr1 = len(original)\n",
    "    lenstr2 = len(fixed)\n",
    "    \n",
    "    d = {}\n",
    "    for i in range(-1, lenstr1+1):\n",
    "        d[(i,-1)] = i+1\n",
    "    for j in range(-1, lenstr2+1):\n",
    "        d[(-1,j)] = j+1\n",
    "\n",
    "    for i in range(lenstr1):\n",
    "        for j in range(lenstr2):\n",
    "            if original[i] == fixed[j]:\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = 1\n",
    "            d[(i,j)] = min(\n",
    "                           d[(i-1,j)] + 1,\n",
    "                           d[(i,j-1)] + 1,\n",
    "                           d[(i-1,j-1)] + cost,\n",
    "                          )\n",
    "            if i and j and original[i] == fixed[j - 1] and original[i - 1] == fixed[j]:\n",
    "                d[(i,j)] = min (d[(i,j)], d[i - 2,j - 2] + cost)\n",
    "    return d[lenstr1-1,lenstr2-1]\n",
    "    \n",
    "\n",
    "def get_error_probability(original: Query, fixed: Query, a=1.5) -> float:\n",
    "    l = lev(original, fixed)\n",
    "    if l == 0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return a ** -l\n",
    "\n",
    "for original, fixed in queries_sample:\n",
    "    p_error = get_error_probability(original, fixed)\n",
    "    print(f'{original:>40s} | {p_error:5.2f} | {fixed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Олтугеза\n",
    "Объедините результат работы предыдущих пунктов в единый алгоритм исправления опечатки для запроса.\n",
    "\n",
    "Примерный план:\n",
    "1.\tДля слов запроса генерируем список ближайших слов-кандидатов (для всех, даже словарных слов).\n",
    "2.\tСобираем список кандидатов-запросов (эвристически, чтобы не сделать экспоненциальное время выполнения)\n",
    "3.\tДля каждого кандидата считаем итоговый объединенный score на основе языковой модели и модели ошибок для данного кандидата (не обязательно сумма или произведение, можно объединение любой сложности).\n",
    "4.\tВыдаём гипотезу с наибольшим score.\n",
    "5.\t???\n",
    "6.\tProfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "def correct0(query: Query) -> Query:\n",
    "    similar_words = []\n",
    "    splitted = query.split()\n",
    "    for word in splitted:\n",
    "        similars = find_similar_words(word)[:4]\n",
    "        similar_words.append(similars)\n",
    "    result = []\n",
    "    for candidate in itertools.product(*similar_words):\n",
    "        q = \" \".join(candidate)\n",
    "        score = get_error_probability(query, q) ** get_probability(q)\n",
    "        result.append((score, q))\n",
    "    \n",
    "    return sorted(result, key=lambda x: x[0])[0][1]\n",
    "\n",
    "\n",
    "def find_filtered_similar_words(word):\n",
    "    similars = find_similar_words(word)[:2]\n",
    "    words = [(get_error_probability(word, w), w) for w in similars]\n",
    "    arr = [w for _, w in sorted(words, key=lambda x: -x[0])]\n",
    "\n",
    "    return [word] if len(arr) == 0 else arr\n",
    "\n",
    "\n",
    "def correct(query: Query) -> Query:\n",
    "    similar_words = []\n",
    "    splitted = query.split()\n",
    "    for word in splitted:\n",
    "        similars = find_filtered_similar_words(word)\n",
    "        similar_words.append(similars)\n",
    "    \n",
    "    result = []\n",
    "    for candidate in itertools.product(*similar_words):\n",
    "        q = \" \".join(candidate)\n",
    "        result.append((-get_probability(q), q))\n",
    "    return sorted(result, key=lambda x: x[0])[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok]                           grand theft auto == grand theft auto\n",
      "[ok]             belarus longitude and latitude == belarus longitude and latitude\n",
      "[ok]                           search for poems == search for poems\n",
      "[fail] lafarge guacotecti dilip resistant prince != large guacamole dip restaurant price\n",
      "[fail]                    texas chainsaw maurer != texas chainsaw massacre\n",
      "[fail]                  royall trumps subtitled != royal tramp subtitle\n",
      "[fail]                floridana fibreglass pols != florida fiberglass pools\n",
      "[fail]                 how to make a calenderer != how to make a calendar\n",
      "[fail]             university of south caroline != university of south carolina\n",
      "[fail]             maureen mcdonald in virginia != maureen mcdonnell in virginia\n"
     ]
    }
   ],
   "source": [
    "for original, fixed in queries_sample:\n",
    "    predict = correct(original)\n",
    "    verdict = '[ok]  ' if predict == fixed else '[fail]'\n",
    "    sign = '==' if predict == fixed else '!='\n",
    "    print(f'{verdict} {predict:>40s} {sign} {fixed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итоговое качество меряем на примерах из `queries.tsv.gz`.\n",
    "\n",
    "Для отладки проблем с качеством имеет смысл научится понимать на каком этапе теряется правильная гипотеза для каждого примера. Например, если правильное исправление есть в списке кандидатов (п. 2), но не выбирается как лучшая – стоит крутить языковую модель, модель ошибок и их объединение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_corrector(queries: Queries, correct: Callable[[Query], Query], debug: bool):\n",
    "    wrong, total = 0, 0\n",
    "    progress = tqdm(queries)\n",
    "    debug_output = 0\n",
    "    for original, fixed in progress:\n",
    "        predict = correct(original)\n",
    "        if predict != fixed:\n",
    "            wrong += 1\n",
    "            if debug:\n",
    "                print(original)\n",
    "                print(fixed)\n",
    "                print(predict)\n",
    "                print()\n",
    "                debug_output += 1\n",
    "                if debug_output == 10:\n",
    "                    break\n",
    "        total += 1\n",
    "        progress.set_description(f'Wrong: {wrong} - {wrong/total*100:0.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ждать все запросы долго, поэтому я перемешал и потестил 10% запросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wrong: 2628 - 26.28%: 100%|██████████| 10000/10000 [57:08<00:00,  2.92it/s] \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "queries_shuffled = queries\n",
    "random.shuffle(queries_shuffled)\n",
    "check_corrector(queries_shuffled[:10000], correct, debug=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
